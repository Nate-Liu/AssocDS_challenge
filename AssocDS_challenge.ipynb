{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AssocDS_Recruitment_chanllenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "# Using 'pip install -r requirements.txt' to install all packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from prophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file \n",
    "DecemberCols = pd.read_csv('Data/DecemberCols.csv', header=None)\n",
    "DecemberAdData = pd.read_csv('Data/DecemberAdData.csv')\n",
    "MarketingCols = pd.read_csv('Data/MarketingCols.csv', header=None)\n",
    "MarketingSales = pd.read_csv('Data/MarketingSales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge December columns name and Data\n",
    "DecemberCols_column_name = DecemberCols.iloc[:,0].tolist()\n",
    "DecemberAdData.columns =  DecemberCols_column_name\n",
    "DecemberAdData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Market columns name and Data\n",
    "Market_column_name = MarketingCols.iloc[:,0].tolist()\n",
    "MarketingSales.columns =  Market_column_name\n",
    "MarketingSales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is NULL value in MarketingSales\n",
    "missing_values_per_column = MarketingSales.isna().sum()\n",
    "print(missing_values_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe to df \n",
    "df = MarketingSales.copy()\n",
    "\n",
    "# Format the Date columns\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%y')\n",
    "#  Extract year from the Date column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df.set_index('Date', inplace=True)  # Set date as the index\n",
    "\n",
    "# Ensure the data is sorted by date\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. seasonality analysis & trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the line plot, faceted by Year, with separate lines for PositiveNews\n",
    "fig = px.line(df, x='Month', y='Sales', color='PositiveNews', \n",
    "              line_group='PositiveNews', # Ensure lines are grouped by  \n",
    "              facet_row='Year', # Create a separate subplot for each year\n",
    "              category_orders={\"Month\": ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                                         'July', 'August', 'September', 'October', 'November', 'December']},\n",
    "              title='Sales Trends by Month and PositiveNews Status Across Years')\n",
    "\n",
    "# Update the layout for better readability\n",
    "fig.update_layout(xaxis_title='Month', yaxis_title='Total Sales')\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1])) # Simplify facet titles\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Now create the line plot, faceted by Year, with separate lines for NegativeCoverage\n",
    "fig = px.line(df, x='Month', y='Sales', color='NegativeCoverage', \n",
    "              line_group='NegativeCoverage', # Ensure lines are grouped by  \n",
    "              facet_row='Year', # Create a separate subplot for each year\n",
    "              category_orders={\"Month\": ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                                         'July', 'August', 'September', 'October', 'November', 'December']},\n",
    "              title='Sales Trends by Month and NegativeCoverage Status Across Years')\n",
    "\n",
    "# Update the layout for better readability\n",
    "fig.update_layout(xaxis_title='Month', yaxis_title='Total Sales')\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1])) # Simplify facet titles\n",
    "fig.update_xaxes(tickangle=-45)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive News Trend Analysis:\n",
    "- There's a noticeable pattern where **sales tend to increase towards the end of the year**, likely around the holiday season, which could suggest a **seasonal effect**.\n",
    "- In years and months with Positive News, we observe a **stronger end-of-year sales increase**, which might indicate that **positive media coverage** is effectively boosting sales during this period.\n",
    "- The increase in sales during the latter months, such as November and December, could be related to **promotional activities or holiday shopping behaviors**.\n",
    "\n",
    "### Negative Coverage Trend Analysis:\n",
    "- In contrast, the graph with Negative Coverage shows **less end-of-year sales spikes** compared to the Positive News graph.\n",
    "- Sales appear to be more consistent throughout the year in the presence of Negative Coverage, with **fewer peaks**.\n",
    "- The dampening of the end-of-year spike could suggest that **negative publicity impacts** the usual seasonal sales boosts.\n",
    "- Interestingly, while Negative Coverage seems to reduce the magnitude of the seasonal peaks, it **doesn't negate the overall upward trend** towards the year's end, indicating **other factors** may also be influencing sales.\n",
    "\n",
    "### Overall Interpretation:\n",
    "- The presence of Positive News correlates with an **accentuated increase in sales towards the end of the year**, reinforcing the idea that **positive media attention can have a beneficial impact** on consumer behavior.\n",
    "- Negative Coverage might **mitigate the positive impact of seasonality on sales**, but it doesn't entirely reverse the seasonal trend, suggesting that sales are influenced by a **complex set of factors** beyond media coverage.\n",
    "- It's essential to consider **other variables not shown in these graphs** that could be affecting sales, such as **marketing campaigns, economic conditions, or changes in product offerings**.\n",
    "- The above graph tells us that **sales tend to spike at the end of year**, this confirms that the sales vary with the ‘Date’ (time) and there is a **seasonality factor** present in our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relationship Between Sales and Advertising Spend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = px.scatter(df, x='AdvertisingSpend', y='Sales', title='Relationship Between Sales and Advertising Spend',\n",
    "                 trendline='ols', trendline_color_override='darkblue')\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(xaxis_title='Advertising Spend', yaxis_title='Sales')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot above visualizes the **relationship between advertising spend and sales**. Each point on the graph represents a data record from the dataset.\n",
    "\n",
    "### Observations:\n",
    "- **Positive Correlation**: There appears to be a **positive correlation** between advertising spend and sales, as indicated by the overall upward trend in the data points. This suggests that higher advertising spend is generally associated with higher sales.\n",
    "- **Data Density**: Most data points are clustered at the lower end of advertising spend, which indicates that there are more records with lower advertising budgets.\n",
    "- **Trendline**: The trendline, shown in dark blue, provides a **linear approximation of the general trend** of the data. It highlights the average effect of advertising spend on sales across the dataset.\n",
    "\n",
    "### Implications:\n",
    "- **Marketing Efficiency**: The pattern observed suggests that there may be an **optimal range of advertising spend** where the return on investment in terms of sales is maximized.\n",
    "- **Budget Allocation**: Companies may consider increasing their advertising budget up to a point where the sales continue to grow proportionately, beyond which the additional spend may yield **diminishing returns**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stationarity of Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Rolling Mean & Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the rolling window size\n",
    "window_size = 30   # A 30-day window for monthly patterns\n",
    "\n",
    "# Create stationarity data\n",
    "# test = df[df['']]\n",
    "# Calculate the rolling mean and rolling st\n",
    "rolling_mean = df['Sales'].rolling(window=window_size).mean()\n",
    "rolling_variance = df['Sales'].rolling(window=window_size).std()\n",
    "\n",
    "# Plotting the original sales data, rolling mean, and rolling variance\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df['Sales'], label='Original Sales', color='blue')\n",
    "plt.plot(rolling_mean, label='Rolling Mean', color='red')\n",
    "plt.plot(rolling_variance, label='Rolling Variance', color='green')\n",
    "plt.title('Sales with Rolling Mean and Standard Deviation')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity of Time Series\n",
    "\n",
    "Analyzing the plot of sales with rolling mean and variance, we observe the following:\n",
    "\n",
    "- The **original sales data** exhibit significant spikes, suggesting potential **seasonal patterns**. The huge drop down at early 2020 as **Covid** happen \n",
    "- The **rolling mean** appears stable, but the spikes indicate possible **seasonal effects**.\n",
    "- The **rolling standard deviation** providing insight into the variability or volatility of the sales data over time.\n",
    "\n",
    "### Conclusion on Stationarity:\n",
    "We can see from the above plot and statistical test that mean and variation doesn’t change much with time, i.e they are constant. Thus, we don’t need to perform any transformation. Then we need to do statistical test to verify if it is stationarity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Dickey-Fuller test (ADF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Augmented Dickey-Fuller test\n",
    "adf_result = adfuller(df['Sales'])\n",
    "\n",
    "print('ADF Statistic:', adf_result[0])\n",
    "print('p-value:', adf_result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in adf_result[4].items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Interpretation of the ADF results\n",
    "if adf_result[0] < adf_result[4][\"5%\"]:\n",
    "    print(\"The time series is stationary as the Test Statistic is lower than the 5% Critical Value.\")\n",
    "else:\n",
    "    print(\"The time series is non-stationary as the Test Statistic is higher than the 5% Critical Value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Augmented Dickey-Fuller (ADF) test has been conducted to determine the stationarity of the time series. The results are as follows:\n",
    "\n",
    "- **ADF Statistic**: The calculated value is approximately `-3.66`.\n",
    "- **p-value**: The test yields a p-value of approximately `0.00468`.\n",
    "- **Critical Values**: The critical value for the `5%` confidence level is approximately `-2.86`.\n",
    "\n",
    "### Stationarity Perspective:\n",
    "\n",
    "- **ADF Statistic vs Critical Values**: The ADF statistic is **lower** than the 5% critical value, which indicates that we can **reject the null hypothesis** of a unit root (non-stationarity) in the time series data.\n",
    "  \n",
    "- **p-value Analysis**: With a p-value **less than 0.05**, we have strong evidence against the null hypothesis, further confirming the **stationarity** of the time series.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "Based on the ADF test results, the time series is considered **stationary** at the 5% significance level, having a **constant mean and variance** over time. This indicates that the time series is free from time-dependent structures such as trends or seasonality, making it suitable for use with forecasting models that assume stationarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 decomposition plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonality and trend using decomposition plots\n",
    "# Perform seasonal decomposition with an additive model, assuming monthly data with a yearly cycle (period=12)\n",
    "result = seasonal_decompose(df['Sales'], model='additive', period=7)\n",
    "\n",
    "# Create data frames for trend, seasonal, and residual components from decomposition\n",
    "trend = result.trend.reset_index()\n",
    "seasonal = result.seasonal.reset_index()\n",
    "resid = result.resid.reset_index()\n",
    "\n",
    "# Plot the Trend Component\n",
    "fig_trend = px.line(trend, x='Date', y='trend', title='Trend Component of Sales')\n",
    "fig_trend.show()\n",
    "\n",
    "# Plot the Seasonal Component\n",
    "fig_seasonal = px.line(seasonal, x='Date', y='seasonal', title='Seasonal Component of Sales')\n",
    "fig_seasonal.show()\n",
    "\n",
    "# Plot the Residual Component\n",
    "fig_resid = px.line(resid, x='Date', y='resid', title='Residual Component of Sales')\n",
    "fig_resid.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time series data has been decomposed into three main components: trend, seasonal, and residual.\n",
    "\n",
    "- **Trend Component**: Shows a smooth progression and provides insight into the underlying long-term trend of the sales data.\n",
    "- **Seasonal Component**: Illustrates the repeating patterns or cycles in the data over specific time intervals, indicating consistent seasonality.\n",
    "- **Residual Component**: Represents the irregularities or 'noise' remaining in the data after the trend and seasonal components have been extracted.\n",
    "\n",
    "### Conclusion on Stationarity:\n",
    "\n",
    "Based on the decomposition, if the **trend component** is relatively flat and the **seasonal component** shows consistent patterns without increasing or decreasing in magnitude, it suggests that the series is stationary with respect to trend and seasonality.\n",
    "\n",
    "The **residual component** should ideally show no pattern and have constant variance, further supporting stationarity. However, if there's significant variance or patterns in the residuals, non-stationarity may still be a concern.\n",
    "\n",
    "Considering both the decomposed components and the ADF test results, the data can be tentatively considered stationary if all conditions are met。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Predictive Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: SARIMAX Time Series Modeling for Sales Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly set frequency, for example daily frequency\n",
    "df = df.asfreq('D')\n",
    "\n",
    "# split into train and test dataset\n",
    "train_size = int(len(df) * 0.9)\n",
    "train = df.iloc[:train_size]\n",
    "test = df.iloc[train_size:]\n",
    "\n",
    "p_values = range(0, 2)\n",
    "d_values = range(0, 2)\n",
    "q_values = range(0, 2)\n",
    "P_values = range(0, 2)\n",
    "D_values = range(0, 2)\n",
    "Q_values = range(0, 2)\n",
    "s_values = [7]\n",
    "\n",
    "# Grid search to find the best parameters\n",
    "best_aic = float('inf')\n",
    "best_params = None\n",
    "for p, d, q, P, D, Q, s in product(p_values, d_values, q_values, P_values, D_values, Q_values, s_values):\n",
    "    try:\n",
    "        model = SARIMAX(train['Sales'], order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "        model_fit = model.fit()\n",
    "        if model_fit.aic < best_aic:\n",
    "            best_aic = model_fit.aic\n",
    "            best_params = (p, d, q, P, D, Q, s)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Train model using the best parameters\n",
    "model = SARIMAX(train['Sales'], order=best_params[:3], seasonal_order=best_params[3:])\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Print the best SARIMA model's AIC and parameters\n",
    "print(f'Best SARIMA Model: AIC={best_aic:.2f}, order={best_params[:3]}, seasonal_order={best_params[3:]}')\n",
    "\n",
    "# Plot the prediction results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train['Sales'], label='Train')\n",
    "plt.plot(test['Sales'], label='Test')\n",
    "plt.plot(model_fit.forecast(len(test)), label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Time Series Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# Calculate error\n",
    "mse = mean_squared_error(test['Sales'], model_fit.forecast(len(test)))\n",
    "rmse = np.sqrt(mse)  # Compute root mean squared error\n",
    "print(f'RMSE: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "- **Forecasting Accuracy**: The predicted values (green) show a projection that initially follows the test data trend but eventually starts to deviate. Notably, the predicted values at the end of the timeline do not capture the upturn observed in the test data (orange).\n",
    "- **RMSE Value**: The RMSE of 15.59 measures the model's forecast error magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking diagnostic plots\n",
    "model_fit.plot_diagnostics(figsize=(10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Standardized Residuals**: The residuals plot does not show any obvious patterns or systematic structure, which is a good indication that the model is capturing the underlying process well.\n",
    "\n",
    "- **Histogram and Estimated Density**: The histogram of standardized residuals suggests a reasonably normal distribution, closely matching the overlaid normal curve (N(0,1)). This indicates that the residuals are approximately normally distributed, meeting one of the key assumptions of many time series models.\n",
    "\n",
    "- **Normal Q-Q Plot**: The Q-Q plot points largely follow the theoretical line, with some deviations on the tails. This indicates that while the residuals are mostly normally distributed, there may be outliers or extreme values not captured by the model.\n",
    "\n",
    "- **Correlogram**: The ACF plot shows that almost all autocorrelations for the residuals are within the confidence interval, suggesting that there is little to no autocorrelation left in the residuals. This implies that the model is accounting well for the time series structure.\n",
    "\n",
    "#### Conclusion\n",
    "The model diagnostics suggest that the SARIMAX model performs adequately, with residuals that are close to normally distributed and no significant autocorrelation. Some minor deviations from normality are noted, which could potentially be improved with further model refinement. However, the current model appears to be a good fit for the data, capturing its main characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Prophet Time Series Modeling for Sales Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Load and prepare the data\n",
    "df_prophet = MarketingSales[['Date', 'Sales']].copy()\n",
    "df_prophet['Date'] = pd.to_datetime(df_prophet['Date'], format='%d/%m/%y')\n",
    "df_prophet = df_prophet.rename(columns={'Date': 'ds', 'Sales': 'y'})\n",
    "\n",
    "# Split the data into train and test sets based on the date\n",
    "cutoff_date = pd.Timestamp('2020-03-01')\n",
    "train = df_prophet[df_prophet['ds'] < cutoff_date]\n",
    "test = df_prophet[df_prophet['ds'] >= cutoff_date]\n",
    "\n",
    "# Create and fit the Prophet model\n",
    "model = Prophet(daily_seasonality=True)\n",
    "model.fit(train)\n",
    "\n",
    "# Make a future dataframe for predictions\n",
    "future_dates = model.make_future_dataframe(periods=test.shape[0])\n",
    "\n",
    "# Predict\n",
    "forecast = model.predict(future_dates)\n",
    "\n",
    "# Plot the forecast along with the actual values\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Configure the figure size before plotting\n",
    "fig = model.plot(forecast, ax=ax)\n",
    "#test.plot(x='ds', y='y', ax=ax, label='Test Data', style='r-')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate RMSE\n",
    "# Extract the predicted yhat values for the test period\n",
    "forecasted_test = forecast[['ds', 'yhat']].merge(test, on='ds')\n",
    "rmse = sqrt(mean_squared_error(forecasted_test['y'], forecasted_test['yhat']))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Optionally, display components of the forecast\n",
    "fig2 = model.plot_components(forecast)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit and Forecast Analysis\n",
    "The Prophet model output showcases several key features:\n",
    "- **Actual Sales Data**: Represented by the black dots, they give a clear indication of historical sales variability and seasonality.\n",
    "- **Forecasted Sales**: The blue line indicates the model's predicted sales values, showing a grasp of the data's underlying patterns and trends.\n",
    "- **Confidence Interval**: The shaded blue area illustrates the forecast's uncertainty, providing a visual representation of prediction confidence.\n",
    "\n",
    "Observations from the plot:\n",
    "- The seasonal peaks and troughs in the historical data are well-captured by the model's forecast.\n",
    "- The model projects an upward trend in sales, suggesting a positive growth expectation. This looks much better compared with SARIMAX Model which has not able to projects an upard trend. \n",
    "\n",
    "### Components of the Forecast\n",
    "The component plots break down the forecast into its constituent parts:\n",
    "- **Trend**: A general increase over time suggests overall sales growth.\n",
    "- **Weekly Seasonality**: Fluctuations in sales during the week could inform marketing strategies and operational planning.\n",
    "- **Yearly/Daily Seasonality**: Insight into periods of high or low activity throughout the year or day, which can aid in long-term planning and resource allocation.\n",
    "\n",
    "### Performance Metric\n",
    "- **Root Mean Squared Error (RMSE)**: The model achieves an RMSE of approximately 19.58, which is a standard metric for evaluating the accuracy of a model's predictions. The effectiveness of the RMSE value is contingent upon the sales data scale.\n",
    "\n",
    "### Conclusion\n",
    "The Prophet model demonstrates a strong capability in capturing the sales data's patterns, with effective trend identification and seasonality projection. The RMSE indicates the model's predictions are relatively accurate, though the actual performance should be contextualized within the specific business environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Forcast December 2020 Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Prophet model to make prediction for the December 2020 Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a future dataframe starting from December 1, 2020, for the month of December\n",
    "# There are 31 days in December 2020\n",
    "future = pd.date_range(start='2020-12-01', periods=31, freq='D')\n",
    "future_df = pd.DataFrame(future, columns=['ds'])\n",
    "\n",
    "# Use the model to make a forecast for December 2020\n",
    "forecast = model.predict(future_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sales prediction for Dec\n",
    "forcast_dec = forecast[['ds', 'yhat']]\n",
    "# Rename \n",
    "forcast_dec = forcast_dec.rename(columns={'ds': 'Date', 'yhat': 'Sales'})\n",
    "\n",
    "# Format the Date columns\n",
    "DecemberAdData['Date'] = pd.to_datetime(DecemberAdData['Date'], format='%d/%m/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# December Prediction\n",
    "Dec = pd.merge(DecemberAdData, forcast_dec, on='Date', how='left')\n",
    "print('Sales Forecast for December 2020:')\n",
    "print(Dec)\n",
    "\n",
    "# Total expected sales for December 2020\n",
    "print('\\nTotal expected sales for December 2020:', Dec['Sales'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# First y-axis\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Sales', color=color)\n",
    "ax1.plot(Dec['Date'], Dec['Sales'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create the second y-axis sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('Advertising Spend', color=color)\n",
    "ax2.plot(Dec['Date'], Dec['AdvertisingSpend'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Add title and grid lines\n",
    "fig.tight_layout()\n",
    "plt.title('Prediction Sales and actual Advertising Spend on Dec 2020')\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Forecast Trend Analysis\n",
    "\n",
    "The blue line on the chart represents the predicted sales over the course of December 2020. Analyzing the trend:\n",
    "\n",
    "- The **predicted sales appear relatively stable** throughout the month, with no dramatic increases or decreases indicated by the forecasting model.\n",
    "- A **slight uptrend** can be observed towards the middle of the month, which could be related to increased consumer activity during the holiday season.\n",
    "- The **highest point** in the predicted sales seems to occur around mid-December, which is a common trend in retail due to the holiday shopping surge.\n",
    "\n",
    "### Strategic Recommendations for the Company\n",
    "\n",
    "Given the expected sales pattern:\n",
    "- **Stock Inventory**: The company should ensure adequate stock levels to meet the anticipated increase in consumer demand, particularly around mid-December.\n",
    "- **Marketing and Promotions**: Aligning marketing efforts with the uptick in sales could further boost revenue; consider launching targeted campaigns just before the peak period.\n",
    "- **Staffing and Support**: Prepare for an influx of customer inquiries and sales activity by adjusting staffing levels and customer service resources accordingly during peak times.\n",
    "- **Post-Peak Strategy**: Develop a post-peak plan to maintain customer engagement and manage excess inventory as the sales trend begins to normalize towards the end of December."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
